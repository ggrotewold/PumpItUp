{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PumpItUp\n",
    "## DrivenDataCompetition PumpItUp\n",
    "- All data is within the same folder as notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1601888635544
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1601888644788
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load training-data and labels\n",
    "trainingValues = pd.read_csv('4910797b-ee55-40a7-8668-10efd5c1b960.csv')\n",
    "trainingLabels = pd.read_csv('0bf8bc6e-30d0-4c50-956a-603fc693d966.csv')\n",
    "trainingData = pd.merge(trainingValues, trainingLabels, on= 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation and Cleansing of data\n",
    "\n",
    "* population == 0 -> Nothing done but maybe an idea for later\n",
    "* num_private Not in feature-description -> delete column\n",
    "* object-data needs to be converted in category-numbers -> done\n",
    "* idea: funders and installers who fund and install regularly are less porbable to create failing pumps than those who do it less often. Thererfore the names are exchanged with the number of occurances. Same for basin, wpt_name, subvillage, scheme_name and schmeme_management. Some names seem to be typos. Therefore a LUP for some of the names created\n",
    "* recorded_by is always value 'geoData Consultants Ltd' -> drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>num_private</th>\n",
       "      <th>region_code</th>\n",
       "      <th>district_code</th>\n",
       "      <th>population</th>\n",
       "      <th>construction_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>5.940000e+04</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37115.131768</td>\n",
       "      <td>317.650385</td>\n",
       "      <td>668.297239</td>\n",
       "      <td>34.077427</td>\n",
       "      <td>-5.706033e+00</td>\n",
       "      <td>0.474141</td>\n",
       "      <td>15.297003</td>\n",
       "      <td>5.629747</td>\n",
       "      <td>179.909983</td>\n",
       "      <td>1300.652475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21453.128371</td>\n",
       "      <td>2997.574558</td>\n",
       "      <td>693.116350</td>\n",
       "      <td>6.567432</td>\n",
       "      <td>2.946019e+00</td>\n",
       "      <td>12.236230</td>\n",
       "      <td>17.587406</td>\n",
       "      <td>9.633649</td>\n",
       "      <td>471.482176</td>\n",
       "      <td>951.620547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-90.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.164944e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18519.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.090347</td>\n",
       "      <td>-8.540621e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37061.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>34.908743</td>\n",
       "      <td>-5.021597e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1986.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>55656.500000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1319.250000</td>\n",
       "      <td>37.178387</td>\n",
       "      <td>-3.326156e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>74247.000000</td>\n",
       "      <td>350000.000000</td>\n",
       "      <td>2770.000000</td>\n",
       "      <td>40.345193</td>\n",
       "      <td>-2.000000e-08</td>\n",
       "      <td>1776.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>30500.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     amount_tsh    gps_height     longitude      latitude  \\\n",
       "count  59400.000000   59400.000000  59400.000000  59400.000000  5.940000e+04   \n",
       "mean   37115.131768     317.650385    668.297239     34.077427 -5.706033e+00   \n",
       "std    21453.128371    2997.574558    693.116350      6.567432  2.946019e+00   \n",
       "min        0.000000       0.000000    -90.000000      0.000000 -1.164944e+01   \n",
       "25%    18519.750000       0.000000      0.000000     33.090347 -8.540621e+00   \n",
       "50%    37061.500000       0.000000    369.000000     34.908743 -5.021597e+00   \n",
       "75%    55656.500000      20.000000   1319.250000     37.178387 -3.326156e+00   \n",
       "max    74247.000000  350000.000000   2770.000000     40.345193 -2.000000e-08   \n",
       "\n",
       "        num_private   region_code  district_code    population  \\\n",
       "count  59400.000000  59400.000000   59400.000000  59400.000000   \n",
       "mean       0.474141     15.297003       5.629747    179.909983   \n",
       "std       12.236230     17.587406       9.633649    471.482176   \n",
       "min        0.000000      1.000000       0.000000      0.000000   \n",
       "25%        0.000000      5.000000       2.000000      0.000000   \n",
       "50%        0.000000     12.000000       3.000000     25.000000   \n",
       "75%        0.000000     17.000000       5.000000    215.000000   \n",
       "max     1776.000000     99.000000      80.000000  30500.000000   \n",
       "\n",
       "       construction_year  \n",
       "count       59400.000000  \n",
       "mean         1300.652475  \n",
       "std           951.620547  \n",
       "min             0.000000  \n",
       "25%             0.000000  \n",
       "50%          1986.000000  \n",
       "75%          2004.000000  \n",
       "max          2013.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe data\n",
    "trainingData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1601888653720
    }
   },
   "outputs": [],
   "source": [
    "# cleanData\n",
    "\n",
    "# function with dropping of missing values\n",
    "def cleanData(data : pd.DataFrame):\n",
    "    data.dropna(inplace = True)    \n",
    "    data = data[data.population != 0] # delete population == 0        \n",
    "    return cleansing(data)\n",
    "\n",
    "# function with substitution of missing values\n",
    "def cleanDataWithoutDropping(data : pd.DataFrame):\n",
    "    data['public_meeting'] = data['public_meeting'].fillna('True') #fill nan with 'True'\n",
    "    data['permit'] = data['permit'].fillna('True')\n",
    "    return cleansing(data)\n",
    "\n",
    "\n",
    "def cleansing(data : pd.DataFrame):\n",
    "    \n",
    "    if 'num_private' in data.columns: # drop 'num_private'\n",
    "        data = data.drop('num_private', axis = 1)\n",
    "    \n",
    "    if 'recorded_by' in data.columns: # drop 'recorded_by'\n",
    "        data = data.drop('recorded_by', axis = 1) \n",
    "    \n",
    "    #clean funder-column\n",
    "    replacementDictionary = {'a/co germany' : 'aco', 'aco/germany': 'aco', 'world vision' : 'worldvision', 'world vision/ kkkt' : 'worldvision', 'world vision/adra' : 'worldvision', \n",
    "                        'world vision/rc church' : 'worldvision', 'women fo partnership' : 'women for partnership', 'acord ngo' : 'acord', 'zao water spring x': 'zao', 'zao water spring' : 'zao',\n",
    "                             'world bank/government' : 'world bank', 'wwf / fores' : 'wwf', 'yasini selemani' : 'yasini', 'adp bungu' : 'adp', 'adp mombo' : 'adp', 'adp/w' : 'adp',\n",
    "                        'private' : 'unknown', 'private institutions': 'unknown', 'not known': 'unknown', 'private individual' : 'unkown', 'de' : 'germany',  'water aid /sema' : ' wateraid',\n",
    "                        'japan government' : 'japan', 'china government': 'china', 'unicef/ csp' : 'unicef',  'private individul' : 'unknown', 'private co' : 'unknown', 'unicef/cspd' : 'unicef', \n",
    "                             'embasy of japan in tanzania' : 'japan' ,'tz japan' : 'japan', 'government of tanzania' : 'tanzania', 'kkkt church' : 'kkkt', 'ms' : 'unknown', 'aic' : 'aict', 'anrikana': 'angrikana',\n",
    "                            'schoo' : 'school', 'school adm9nstrarion' : 'school', 'school administration' : 'school', 'school capital':'school', 'Serikali Ya Kijiji' : 'serikali', 'swedish tandala project' : 'swedish', \n",
    "                             'swiss if':'swiss', 'swisland/ mount meru flowers':'swisland', 'tanzania and egypt cooperat' : 'tanzania', 'tanzania christian service' : 'tanzania', 'tanzania na egypt' : 'tanzania', \n",
    "                             'tanzania nea egypt' : 'tanzania', 'tassaf i' : 'tassaf', 'tassaf ii' : 'tassaf', 'tassaf/ danida' : 'tassaf', 'tanzania/australia' : 'tanzania', 'tcrs /care' : 'tcrs', 'tcrs /government' : 'tcrs', \n",
    "                             'tcrs/village community' : 'tcrs', 'unice' : 'unicef'}\n",
    "    data['funder'] = data['funder'].astype(str)\n",
    "    data['funder'] = data['funder'].apply(lambda x: x.strip().lower())\n",
    "    data['funder'] = data['funder'].replace(replacementDictionary)\n",
    "    data['funder'] = data['funder'].astype('object')\n",
    "\n",
    "        \n",
    "        \n",
    "    # list of columns\n",
    "    columns = ['funder','installer','wpt_name','basin', 'subvillage','scheme_name', 'scheme_management']\n",
    "    \n",
    "    for columnName in columns: # make all strings lower case and replace names with number of occurence.\n",
    "        data[columnName] = data[columnName].fillna('unknown')\n",
    "        data.loc[data[columnName].str.len() <=2, columnName] = 'unknown'\n",
    "        data.loc[data[columnName] == 'none', columnName] = 'unknown'\n",
    "        data[columnName].str.lower()\n",
    "        data = data.replace({columnName : data[columnName].value_counts().to_dict()})\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    #change data-recorded to year- and month-only-columns\n",
    "    data['date_recorded'] = pd.to_datetime(data['date_recorded'], format='%Y-%m-%d')\n",
    "    data['year_recorded'] =data['date_recorded'].dt.year\n",
    "    data['year_recorded'] = data['year_recorded'].astype('object')\n",
    "\n",
    "    data['month_recorded'] = data['date_recorded'].dt.month\n",
    "    data['month_recorded'] = data['month_recorded'].astype('object')\n",
    "    \n",
    "    data['day_recorded'] = data['date_recorded'].dt.day\n",
    "    data['day_recorded'] = data['day_recorded'].astype('object')\n",
    "\n",
    "    data = data.drop('date_recorded', axis = 1)\n",
    "    \n",
    "    #change labels to category data\n",
    "    cat_columns = data.select_dtypes(['object']).columns\n",
    "    if len(cat_columns) == 0:\n",
    "        return data\n",
    "    data[cat_columns] = data[cat_columns].astype('category')\n",
    "    cat_data = data[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    data = data.join(cat_data, rsuffix='_cat')\n",
    "    \n",
    "        \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#check correlation\n",
    "import seaborn as sns\n",
    "data = cleanData(trainingData)\n",
    "fig = plt.pyplot.gcf() \n",
    "fig.set_size_inches(25,25)\n",
    "data = pd.DataFrame(data.corr()['status_group_cat'])\n",
    "no_use_columns = data.select_dtypes(['category']).columns\n",
    "data = data.drop(no_use_columns, axis=1)\n",
    "\n",
    "sns.heatmap(data, annot = True, fmt='g',cmap= 'coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1601889400410
    }
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "def prepareData(data, size_TestData):\n",
    "    #split in labels and features\n",
    "    labels = data['status_group_cat']\n",
    "    features = data.drop(['status_group_cat','id'],axis=1)\n",
    "    no_use_columns = features.select_dtypes(['category']).columns\n",
    "    features = features.drop(no_use_columns, axis=1)\n",
    "    features_list = features.columns\n",
    "\n",
    "    #split labels and features is train-data and test-data\n",
    "    if size_TestData == 1.0:\n",
    "        return features, _, labels, _\n",
    "    else: \n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = size_TestData, random_state = 42)\n",
    "        return train_features, test_features, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1601143755281
    }
   },
   "outputs": [],
   "source": [
    "#Experiment 1: Deleting works better than substitution\n",
    "# (Accuracy 0.8343399482312338 in comparison to 0.8146801346801347)\n",
    "DONE = True\n",
    "if DONE is False:\n",
    "    randomForest = RandomForestClassifier(n_estimators= 1600, min_samples_split= 5, min_samples_leaf= 1, max_depth= 100, bootstrap= True, max_features= 'auto')\n",
    "    \n",
    "\n",
    "    \n",
    "    trainingDataV1 = cleanDataV1(trainingData.copy(), ['funder', 'installer','wpt_name','basin', 'subvillage','scheme_name', 'scheme_management'])\n",
    "    print('dataV1 cleaned')\n",
    "    trainingDataV2 = cleanDataV2(trainingData.copy(), ['funder', 'installer','wpt_name','basin', 'subvillage','scheme_name', 'scheme_management'])\n",
    "    print('dataV2 cleaned')\n",
    "    train_features, test_features, train_labels, test_labels = prepareData(trainingDataV1)\n",
    "    print('dataV1 prepared')\n",
    "    randomForest.fit(train_features, train_labels)\n",
    "    predictions = randomForest.predict(test_features)\n",
    "    accuracyV1 = sklearn.metrics.accuracy_score(test_labels, predictions)\n",
    "    print(\"accuracyV1: \", accuracyV1)\n",
    "    train_features, test_features, train_labels, test_labels = prepareData(trainingDataV2)\n",
    "    print ('dataV2 prepared')\n",
    "    randomForest.fit(train_features, train_labels)\n",
    "    predictions = randomForest.predict(test_features)\n",
    "    accuracyV2 = sklearn.metrics.accuracy_score(test_labels, predictions)\n",
    "    print(\"accuracyV2: \", accuracyV2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is cleaned\n",
      "ERROR: Shape of data failed\n",
      "data is prepared\n",
      "ERROR: Shape of data failed\n",
      "ERROR: Shape of data failed\n",
      "ERROR: Shape of data failed\n",
      "AccuarcyV1:  0.7424915824915825  True, False\n",
      "AccuarcyV2:  0.7424915824915825  False, True\n",
      "AccuarcyV3:  0.7424915824915825  False, False\n"
     ]
    }
   ],
   "source": [
    "#Experiment 2: Which substitution values work better for prediction?\n",
    "# Values for 'public_meeting' and 'permit' have no effect on accuracy. Can be set to any value.\n",
    "    \n",
    "DONE = True\n",
    "if DONE is False:\n",
    "    randomForest = RandomForestClassifier(n_estimators= 1600, min_samples_split= 5, min_samples_leaf= 1, max_depth= 100, bootstrap= True, max_features= 'auto')\n",
    "    data = cleanData(trainingData.copy(), ['funder', 'installer','wpt_name','basin', 'subvillage','scheme_name', 'scheme_management'])\n",
    "    print('data is cleaned')\n",
    "    train_features, test_features, train_labels, test_labels = prepareData(data)\n",
    "    print('data is prepared')\n",
    "    \n",
    "    randomForest.fit(train_features, train_labels)\n",
    "    \n",
    "    testDataV1 = cleanDataWithoutDropping(trainingData.copy(), ['funder', 'installer','wpt_name','basin', 'subvillage','scheme_name', 'scheme_management'], True, False)\n",
    "    testDataV2 = cleanDataWithoutDropping(trainingData.copy(), ['funder', 'installer','wpt_name','basin', 'subvillage','scheme_name', 'scheme_management'], False, True)\n",
    "    testDataV3 = cleanDataWithoutDropping(trainingData.copy(), ['funder', 'installer','wpt_name','basin', 'subvillage','scheme_name', 'scheme_management'], False, False)\n",
    "    \n",
    "    _, prepared_test_featuresV1, _, prepared_test_labelsV1 = prepareData(testDataV1)\n",
    "    _, prepared_test_featuresV2, _, prepared_test_labelsV2 = prepareData(testDataV2)\n",
    "    _, prepared_test_featuresV3, _, prepared_test_labelsV3 = prepareData(testDataV3)\n",
    "    \n",
    "    predictions = randomForest.predict(prepared_test_featuresV1)\n",
    "    accuracyV1 = sklearn.metrics.accuracy_score(prepared_test_labelsV1, predictions)\n",
    "     \n",
    "    predictions = randomForest.predict(prepared_test_featuresV2)\n",
    "    accuracyV2 = sklearn.metrics.accuracy_score(prepared_test_labelsV2, predictions)\n",
    "    \n",
    "    predictions = randomForest.predict(prepared_test_featuresV3)\n",
    "    accuracyV3 = sklearn.metrics.accuracy_score(prepared_test_labelsV3, predictions)\n",
    "    \n",
    "    print('AccuarcyV1: ', accuracyV1, ' True, False')\n",
    "    print('AccuarcyV2: ', accuracyV1, ' False, True')\n",
    "    print('AccuarcyV3: ', accuracyV1, ' False, False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Shape of data failed:  (44550, 40) (44550,) (14850, 40) (14850,)\n",
      "Accuracy with numberOfOccurances is:  0.8145454545454546\n",
      "ERROR: Shape of data failed:  (44550, 40) (44550,) (14850, 40) (14850,)\n",
      "Accuracy without numberOfOccurances is:  0.8138720538720539\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3: Does use of frequency for funder, installer, etc gain accuracy? test-level of accuracy is 0.7424915824915825\n",
    "# Gain in accuarcy by use of 'number of occurances' as category \n",
    "DONE = True\n",
    "\n",
    "def cleanData(data : pd.DataFrame, columns):\n",
    "    if 'num_private' in data.columns: # drop 'num_private'\n",
    "        data = data.drop('num_private', axis = 1)\n",
    "    \n",
    "    data['public_meeting'] = data['public_meeting'].fillna('True')\n",
    "    data['permit'] = data['permit'].fillna('False')\n",
    "    \n",
    "    for columnName in columns: # make all strings lower case and replace names with number of occurence.\n",
    "        data[columnName] = data[columnName].fillna('unknown')\n",
    "        data.loc[data[columnName].str.len() <=2, columnName] = 'unknown'\n",
    "        data.loc[data[columnName] == 'none', columnName] = 'unknown'\n",
    "        data[columnName].str.lower()\n",
    "        data[columnName] = data[columnName].astype('object')\n",
    "        \n",
    "    #change data-recorded to year- and month-only-columns\n",
    "    data['date_recorded'] = pd.to_datetime(data['date_recorded'], format='%Y-%m-%d')\n",
    "    data['year_recorded'] =data['date_recorded'].dt.year\n",
    "    data['year_recorded'] = data['year_recorded'].astype('object')\n",
    "\n",
    "    data['month_recorded'] = data['date_recorded'].dt.month\n",
    "    data['month_recorded'] = data['month_recorded'].astype('object')\n",
    "    \n",
    "    data['day_recorded'] = data['date_recorded'].dt.day\n",
    "    data['day_recorded'] = data['day_recorded'].astype('object')\n",
    "\n",
    "    data = data.drop('date_recorded', axis = 1)\n",
    "    \n",
    "    dataV2 = data.copy()\n",
    "    \n",
    "    for columnName in columns:\n",
    "        data = data.replace({columnName : data[columnName].value_counts().to_dict()})\n",
    "    \n",
    "    #change labels to category data\n",
    "    cat_columns = data.select_dtypes(['object']).columns\n",
    "    data[cat_columns] = data[cat_columns].astype('category')\n",
    "    cat_data = data[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    data = data.join(cat_data, rsuffix='_cat')\n",
    "    \n",
    "    cat_columns = dataV2.select_dtypes(['object']).columns\n",
    "    dataV2[cat_columns] = dataV2[cat_columns].astype('category')\n",
    "    cat_data = dataV2[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    dataV2 = dataV2.join(cat_data, rsuffix='_cat')\n",
    "    \n",
    "        \n",
    "    return data, dataV2\n",
    "\n",
    "if DONE is not True:\n",
    "    randomForest = RandomForestClassifier(n_estimators= 1600, min_samples_split= 5, min_samples_leaf= 1, max_depth= 100, bootstrap= True, max_features= 'auto')\n",
    "    data, dataV2 = cleanData(trainingData.copy(), ['funder', 'installer','wpt_name','basin', 'subvillage','scheme_name', 'scheme_management'])\n",
    "    train_features, test_features, train_labels, test_labels = prepareData(data)\n",
    "    randomForest.fit(train_features, train_labels)\n",
    "    predictions = randomForest.predict(test_features)\n",
    "    print('Accuracy with numberOfOccurances is: ', sklearn.metrics.accuracy_score(test_labels, predictions))\n",
    "    train_features, test_features, train_labels, test_labels = prepareData(dataV2)\n",
    "    randomForest.fit(train_features, train_labels)\n",
    "    predictions = randomForest.predict(test_features)\n",
    "    print('Accuracy without numberOfOccurances is: ', sklearn.metrics.accuracy_score(test_labels, predictions))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1601889608886
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#create features and label\n",
    "data = cleanDataWithoutDropping(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1601889624728
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59400, 39)   (59400,)\n"
     ]
    }
   ],
   "source": [
    "preparedData_features, _,preparedData_labels, _ = prepareData(data, 1.0)\n",
    "print(preparedData_features.shape, \" \", preparedData_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1600610464668
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed: 12.0min\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 15.0min\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed: 19.3min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed: 19.6min\n"
     ]
    }
   ],
   "source": [
    "## Try random fit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "##create random grid\n",
    "# number of trees in forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 500, stop= 2000, num = 100)]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 4, 5, 6, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 3, 4]\n",
    "# Method for selecting samples for training each node\n",
    "bootstrap = [True, False]\n",
    "# max features\n",
    "max_features = ['auto', 'log2', 'sqrt']\n",
    "\n",
    "\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'max_features': max_features,\n",
    "              'bootstrap': bootstrap}\n",
    "\n",
    "randomforest = RandomForestClassifier()\n",
    "\n",
    "randomforest_random = RandomizedSearchCV(estimator = randomforest, param_distributions=random_grid, n_iter = 50, cv= None, verbose= 20, random_state= 42, n_jobs = -1)\n",
    "randomforest_random.fit(preparedData_features, preparedData_labels)\n",
    "best_estimator = randomforest_random.best_estimator_\n",
    "\n",
    "# Show parameters\n",
    "print('Best parameters: ',randomforest_random.best_params_)\n",
    "\n",
    "# Show Accuracy\n",
    "predictions = best_estimator.predict(test_features)\n",
    "print ('Accuracy-Score of randomforest is: ', sklearn.metrics.accuracy_score(test_labels, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1601895833874
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "competition_features shape:  (14850, 40)\n",
      "competition features prepared shape:  (14850, 39)\n",
      "shape predictions (14850,)\n",
      "shape competition_result (14850, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def competitionOutput(filenameIn, filenameOut ,estimator):\n",
    "    # prepare competition features\n",
    "    competition_features = pd.read_csv(filenameIn)\n",
    "    print('competition_features shape: ',competition_features.shape)\n",
    "    competition_features_prepared = cleanDataWithoutDropping(competition_features)\n",
    "    competition_features_prepared = competition_features_prepared.drop(['id'],axis=1)\n",
    "    no_use_columns = competition_features_prepared.select_dtypes(['category']).columns\n",
    "    competition_features_prepared = competition_features_prepared.drop(no_use_columns, axis=1)\n",
    "    \n",
    "    print('competition features prepared shape: ', competition_features_prepared.shape)\n",
    "\n",
    "    predictions = estimator.predict(competition_features_prepared)\n",
    "    print('shape predictions', predictions.shape)\n",
    "    competition_result = pd.read_csv(filenameIn)\n",
    "    print('shape competition_result', competition_result.shape)\n",
    "    competition_result['status_group'] = predictions\n",
    "    competition_result = competition_result[['id', 'status_group']]\n",
    "    competition_result['status_group'] = competition_result['status_group'].replace([0,1,2],['functional','functional needs repair', 'non functional'])\n",
    "\n",
    "    # create output csv\n",
    "    competition_result.to_csv(filenameOut, index=False)\n",
    "    return competition_features_prepared\n",
    "\n",
    "competition_features_prepared = competitionOutput('702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv', 'predictionsV5.csv', best_estimator)\n",
    "competition_features_prepared.columns[competition_features_prepared.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
